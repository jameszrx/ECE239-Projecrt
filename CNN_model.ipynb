{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "#from tensorflow.python.ops import rnn, rnn_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(i):\n",
    "    a = [0,0,0,0]\n",
    "    a[i-769] = 1;\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 1000, 22)\n",
      "(288, 1000, 22)\n",
      "(288, 1000, 22)\n",
      "(288, 1000, 22)\n",
      "(288, 1000, 22)\n",
      "(288, 1000, 22)\n",
      "(288, 1000, 22)\n",
      "(288, 1000, 22)\n",
      "(288, 1000, 22)\n"
     ]
    }
   ],
   "source": [
    "# Load subject data and store the reshaped data in X_sub[i], y_sub[i] \n",
    "X_sub = []\n",
    "y_sub = []\n",
    "for i in np.arange(9):\n",
    "    data = h5py.File('./project_datasets/project_datasets/A0'+str(i+1)+'T_slice.mat', 'r')\n",
    "    X_sub.append(np.copy(data['image']))\n",
    "    y_sub.append(np.copy(data['type']))\n",
    "    X_sub[i] = X_sub[i][:, :22, :]\n",
    "    X_sub[i] = X_sub[i].transpose([0,2,1])\n",
    "    y_sub[i] = y_sub[i][0,0:X_sub[i].shape[0]:1]\n",
    "    y_sub[i] = np.asarray(y_sub[i], dtype=np.int32)\n",
    "    y_sub[i] = [one_hot(j) for j in y_sub[i]]\n",
    "    y_sub[i] = np.asarray(y_sub[i], dtype = np.int8)\n",
    "    print(X_sub[i].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN\n",
    "for i in np.arange(9):\n",
    "    y_sub[i] = y_sub[i][~np.isnan(X_sub[i]).any(axis=(1,2))]\n",
    "    X_sub[i] = X_sub[i][~np.isnan(X_sub[i]).any(axis=(1,2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization\n",
    "def normalize(dataset):\n",
    "    mu = np.mean(dataset,axis=0)\n",
    "    sigma = np.std(dataset,axis=0)\n",
    "    return (dataset - mu)/ sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2046, 1000, 22)\n",
      "(2046, 20, 1100)\n",
      "(2046, 4)\n",
      "(512, 1000, 22)\n",
      "(512, 20, 1100)\n",
      "(512, 4)\n",
      "(512000, 4)\n"
     ]
    }
   ],
   "source": [
    "# For training across all subjects\n",
    "# Shuffle the data, sample 20% for testing, and the rest for training\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "X = np.concatenate([X_sub[i] for i in np.arange(9)])\n",
    "y = np.concatenate([y_sub[i] for i in np.arange(9)])\n",
    "num_trial = X.shape[0]\n",
    "index_shuffle = np.arange(num_trial)\n",
    "np.random.shuffle(index_shuffle)\n",
    "X_shuffle = X[index_shuffle]\n",
    "y_shuffle = y[index_shuffle]\n",
    "\n",
    "training_index = int(np.floor(num_trial * (1 - 0.2)))\n",
    "#validation_index = training_index + int(np.floor(num_trial * 0.2))\n",
    "# test_index = num_trial\n",
    "\n",
    "X_train = X_shuffle[0:training_index]\n",
    "y_train = y_shuffle[0:training_index]\n",
    "X_train_new_shape = X_train.reshape([X_train.shape[0], 20, 50, 22]).reshape([X_train.shape[0], 20, -1])\n",
    "# X_val = X_shuffle[training_index:validation_index]\n",
    "# y_val = y_shuffle[training_index:validation_index]\n",
    "# y_val_steps = np.tile(y_val,((X_train.shape[1]),1))\n",
    "X_test = X_shuffle[training_index:num_trial]\n",
    "X_test_new_shape = X_test.reshape([X_test.shape[0], 20, 50, 22]).reshape([X_test.shape[0], 20, -1])\n",
    "y_test = y_shuffle[training_index:num_trial]\n",
    "y_test_steps = np.tile(y_test,((X_train.shape[1]),1))\n",
    "\n",
    "X_train = normalize(X_train)\n",
    "X_train_new_shape = normalize(X_train_new_shape)\n",
    "#X_val = normalize(X_val)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train_new_shape.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_test_new_shape.shape)\n",
    "print(y_test.shape)\n",
    "print(y_test_steps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237, 1000, 22)\n",
      "(237, 20, 1100)\n",
      "(50, 1000, 22)\n",
      "(50, 20, 1100)\n",
      "(50000, 4)\n",
      "\n",
      "(236, 1000, 22)\n",
      "(236, 20, 1100)\n",
      "(50, 1000, 22)\n",
      "(50, 20, 1100)\n",
      "(50000, 4)\n",
      "\n",
      "(236, 1000, 22)\n",
      "(236, 20, 1100)\n",
      "(50, 1000, 22)\n",
      "(50, 20, 1100)\n",
      "(50000, 4)\n",
      "\n",
      "(234, 1000, 22)\n",
      "(234, 20, 1100)\n",
      "(50, 1000, 22)\n",
      "(50, 20, 1100)\n",
      "(50000, 4)\n",
      "\n",
      "(232, 1000, 22)\n",
      "(232, 20, 1100)\n",
      "(50, 1000, 22)\n",
      "(50, 20, 1100)\n",
      "(50000, 4)\n",
      "\n",
      "(235, 1000, 22)\n",
      "(235, 20, 1100)\n",
      "(50, 1000, 22)\n",
      "(50, 20, 1100)\n",
      "(50000, 4)\n",
      "\n",
      "(238, 1000, 22)\n",
      "(238, 20, 1100)\n",
      "(50, 1000, 22)\n",
      "(50, 20, 1100)\n",
      "(50000, 4)\n",
      "\n",
      "(232, 1000, 22)\n",
      "(232, 20, 1100)\n",
      "(50, 1000, 22)\n",
      "(50, 20, 1100)\n",
      "(50000, 4)\n",
      "\n",
      "(228, 1000, 22)\n",
      "(228, 20, 1100)\n",
      "(50, 1000, 22)\n",
      "(50, 20, 1100)\n",
      "(50000, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For training within each subject[i]\n",
    "# Shuffle the data, sample 50 trials for testing, and the rest for training -->\n",
    "X_train_sub = []\n",
    "X_train_new_shape_sub = []\n",
    "\n",
    "y_train_sub = []\n",
    "y_train_new_shape_sub = []\n",
    "\n",
    "X_test_sub = []\n",
    "X_test_new_shape_sub = []\n",
    "\n",
    "y_test_sub = []\n",
    "y_test_new_shape_sub = []\n",
    "\n",
    "y_test_steps_sub = []\n",
    "\n",
    "for i in np.arange(9):\n",
    "    num_trial_sub = X_sub[i].shape[0]\n",
    "    training_index_sub = num_trial_sub - 50\n",
    "    \n",
    "    index_shuffle_sub = np.arange(num_trial_sub)\n",
    "    np.random.shuffle(index_shuffle_sub)\n",
    "    \n",
    "    X_shuffle_sub = X_sub[i][index_shuffle_sub]\n",
    "    y_shuffle_sub = y_sub[i][index_shuffle_sub]\n",
    "    X_train_sub.append(X_shuffle_sub[0:training_index_sub])\n",
    "    y_train_sub.append(y_shuffle_sub[0:training_index_sub])\n",
    "    X_test_sub.append(X_shuffle[training_index_sub:num_trial_sub])\n",
    "    y_test_sub.append(y_shuffle[training_index_sub:num_trial_sub])\n",
    "    \n",
    "    # new shape\n",
    "    \n",
    "    X_shuffle_new_shape_sub = X_sub[i][index_shuffle_sub].reshape([-1, 20, 50, 22]).reshape([-1, 20, 1100])\n",
    "    X_train_new_shape_sub.append(X_shuffle_new_shape_sub[0:training_index_sub])\n",
    "    X_test_new_shape_sub.append(X_shuffle_new_shape_sub[training_index_sub:num_trial_sub])\n",
    "    '''\n",
    "    y_shuffle_new_shape_sub = y_sub[i][index_shuffle_sub].reshape([-1, 20, 50, 22]).reshape([-1, 20, 1100])\n",
    "    y_train_new_shape_sub.append(y_shuffle_new_shape_sub[0:training_index_sub])\n",
    "    y_test_new_shape_sub.append(y_shuffle_new_shape_sub[training_index_sub:num_trial_sub])\n",
    "    '''\n",
    "    \n",
    "    y_test_steps_sub.append(np.tile(y_test_sub[i],((X_train_sub[i].shape[1]),1)))\n",
    "\n",
    "    X_train_sub[i] = normalize(X_train_sub[i])\n",
    "    #X_train_new_shape_sub[i] = normalize(X_train_new_shape_sub[i])\n",
    "    X_test_sub[i] = normalize(X_test_sub[i])\n",
    "    X_test_new_shape_sub[i] = normalize(X_test_new_shape_sub[i])\n",
    "    \n",
    "    print(X_train_sub[i].shape)\n",
    "    print(X_train_new_shape_sub[i].shape)\n",
    "    print(X_test_sub[i].shape)\n",
    "    print(X_test_new_shape_sub[i].shape)\n",
    "    \n",
    "    print(y_test_steps_sub[i].shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "# For training across all subjects: Hyperparameters setting\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 30\n",
    "batch_size = 35\n",
    "total_batches = (X_train.shape[0]//batch_size)\n",
    "print(total_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters about CNN model from paper\n",
    "# idea: input of CovLayer is a 2d matrix with size 1000 * 22\n",
    "image_height = 1000\n",
    "image_width = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders\n",
    "x_holder = tf.placeholder(tf.float32, [None, image_height, image_width, 1])\n",
    "y_holder = tf.placeholder(tf.float32, [None, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 1: 25, 10 * 1 filter + 25, 25 * 22 filter + maxpool 3*1\n",
    "layer1_params = {\n",
    "    'filter_set_1':[10, 1, 1, 25],\n",
    "    'filter_set_2':[1, 22, 25, 25]\n",
    "}\n",
    "\n",
    "W1_1 = tf.Variable(tf.random_normal(layer1_params['filter_set_1'], stddev=0.1))\n",
    "b1_1 = tf.Variable(tf.random_normal([25], stddev=0.1))\n",
    "W1_2 = tf.Variable(tf.random_normal(layer1_params['filter_set_2'], stddev=0.1))\n",
    "b1_2 = tf.Variable(tf.random_normal([25], stddev=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 991, 22, 25)\n",
      "(?, 991, 1, 25)\n",
      "(?, 330, 1, 25)\n"
     ]
    }
   ],
   "source": [
    "# Define computation graph - layer 1\n",
    "a1_1 = tf.nn.conv2d(x_holder, W1_1, strides=[1,1,1,1], padding='VALID') + b1_1\n",
    "print(a1_1.shape)\n",
    "a1_2 = tf.nn.conv2d(a1_1, W1_2, strides=[1,1,1,1], padding='VALID') + b1_2\n",
    "print(a1_2.shape)\n",
    "z1 = tf.nn.max_pool(a1_2, ksize=[1,3,1,1], strides=[1,3,1,1], padding='VALID')\n",
    "print(z1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 321, 1, 50)\n",
      "(?, 321, 1, 50)\n",
      "(?, 160, 1, 50)\n"
     ]
    }
   ],
   "source": [
    "# Define computation graph - layer 2\n",
    "layer2_params = {\n",
    "    'filter':[10,1,25,50],\n",
    "    'max_pool':[1,2,1,1]\n",
    "}\n",
    "W2 = tf.Variable(tf.random_normal(layer2_params['filter'],stddev=0.1))\n",
    "b2 = tf.Variable(tf.random_normal([50]))\n",
    "a2 = tf.nn.conv2d(z1, W2, strides=[1,1,1,1], padding='VALID') + b2\n",
    "print(a2.shape)\n",
    "h2 = tf.nn.relu(a2)\n",
    "print(h2.shape)\n",
    "z2 = tf.nn.max_pool(h2, ksize=layer2_params['max_pool'], strides=[1,2,1,1], padding='VALID')\n",
    "print(z2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 151, 1, 100)\n",
      "(?, 151, 1, 100)\n",
      "(?, 75, 1, 100)\n"
     ]
    }
   ],
   "source": [
    "# Define computation graph - layer 3\n",
    "layer3_params = {\n",
    "    'filter':[10, 1, 50, 100],\n",
    "    'max_pool':[1,2,1,1]\n",
    "}\n",
    "W3 = tf.Variable(tf.random_normal(layer3_params['filter'],stddev=0.1))\n",
    "b3 = tf.Variable(tf.random_normal([100]))\n",
    "a3 = tf.nn.conv2d(z2, W3, strides=[1,1,1,1], padding='VALID') + b3\n",
    "print(a3.shape)\n",
    "h3 = tf.nn.relu(a3)\n",
    "print(h3.shape)\n",
    "z3 = tf.nn.max_pool(h3, ksize=layer3_params['max_pool'], strides=[1,2,1,1], padding='VALID')\n",
    "print(z3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 66, 1, 200)\n",
      "(?, 66, 1, 200)\n",
      "(?, 33, 1, 200)\n"
     ]
    }
   ],
   "source": [
    "# Define computation graph - layer 4\n",
    "layer4_params = {\n",
    "    'filter':[10, 1, 100, 200],\n",
    "    'max_pool':[1,2,1,1]\n",
    "}\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal(layer4_params['filter'],stddev=0.1))\n",
    "b4 = tf.Variable(tf.random_normal([200]))\n",
    "a4 = tf.nn.conv2d(z3, W4, strides=[1,1,1,1], padding='VALID') + b4\n",
    "print(a4.shape)\n",
    "h4 = tf.nn.relu(a4)\n",
    "print(h4.shape)\n",
    "z4 = tf.nn.max_pool(h4, ksize=layer4_params['max_pool'], strides=[1,2,1,1], padding='VALID')\n",
    "print(z4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4)\n"
     ]
    }
   ],
   "source": [
    "# Define computation graph - fc layer\n",
    "num_features = z4.get_shape()[1:4].num_elements()\n",
    "flaten_layer = tf.reshape(z4, [-1, num_features])\n",
    "\n",
    "W5 = tf.Variable(tf.random_normal(shape = [num_features, 4], stddev=0.1))\n",
    "b5 = tf.Variable(tf.random_normal(shape = [4], stddev=0.1))\n",
    "h5 = tf.matmul(flaten_layer, W5) + b5\n",
    "z_out = tf.nn.relu(h5)\n",
    "print(z_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute loss\n",
    "y_pred = tf.nn.softmax(z_out)\n",
    "y_pred_class = tf.argmax(y_pred, axis=1)\n",
    "cross_entropy = tf.losses.softmax_cross_entropy(y_holder, logits=z_out)\n",
    "regularizer = tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3) + tf.nn.l2_loss(W4)\n",
    "cost = tf.reduce_mean(cross_entropy + 0.05 * regularizer)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(y_pred, axis = 1),tf.argmax(y_holder,axis = 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy = 0.31, training loss = 73.997986\n",
      "training accuracy = 0.14, training loss = 65.938446\n",
      "training accuracy = 0.31, training loss = 64.804619\n",
      "Epoch 0: test accuracy = 0.236328\n",
      "training accuracy = 0.23, training loss = 64.426697\n",
      "training accuracy = 0.14, training loss = 63.213554\n",
      "training accuracy = 0.31, training loss = 61.971409\n",
      "Epoch 1: test accuracy = 0.236328\n",
      "training accuracy = 0.23, training loss = 61.570477\n",
      "training accuracy = 0.14, training loss = 60.312523\n",
      "training accuracy = 0.31, training loss = 59.052643\n",
      "Epoch 2: test accuracy = 0.236328\n",
      "training accuracy = 0.23, training loss = 58.650120\n",
      "training accuracy = 0.14, training loss = 57.396446\n",
      "training accuracy = 0.31, training loss = 56.151993\n",
      "Epoch 3: test accuracy = 0.236328\n",
      "training accuracy = 0.23, training loss = 55.756180\n",
      "training accuracy = 0.14, training loss = 54.528118\n",
      "training accuracy = 0.31, training loss = 53.314800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-208-3214f0a169a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx_holder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_holder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training accuracy = %.2f, training loss = %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "display_step = 25\n",
    "'''\n",
    "sample_x = X_train[15:50, :]\n",
    "batch_shape = sample_x.shape\n",
    "print(sample_x.shape)\n",
    "batch_x = sample_x.reshape([batch_shape[0], batch_shape[1], batch_shape[2], 1])\n",
    "print(batch_x.shape)\n",
    "\n",
    "'''\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (y_train.shape[0] - batch_size)\n",
    "            sample_x = X_train[offset:(offset + batch_size), :]\n",
    "            batch_shape = sample_x.shape\n",
    "            batch_x = sample_x.reshape([batch_shape[0], batch_shape[1], batch_shape[2], 1])\n",
    "            batch_y = y_train[offset:(offset + batch_size), :]\n",
    "            \n",
    "            _, acc, loss = sess.run([optimizer, accuracy, cost], feed_dict={x_holder: batch_x, y_holder: batch_y})\n",
    "            if (b % display_step == 0):\n",
    "                print('training accuracy = %.2f, training loss = %f' % (acc, loss))\n",
    "        test_shape = X_test.shape\n",
    "        x_feed = X_test.reshape([test_shape[0], test_shape[1], test_shape[2], 1])\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={x_holder:x_feed, y_holder:y_test})\n",
    "        print('Epoch %d: test accuracy = %f' % (epoch, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
